{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can open this notebook in Colab by clicking the Colab icon. Colab provides GPU for free. You can also run this notebook locally by installing the dependencies listed in `requirements.txt`.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osbm/unet_explainer/blob/main/tutorial-part1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "First part of this hands-on will show you a working example of pretrained deep learning model on semantic segmentation on prostate images, in the context of 2D multi-slice prostate MR images segmentation. You will use the (prostate-158 test dataset)[https://zenodo.org/record/6592345] dataset (19 mri images).\n",
    "\n",
    "### Objectives of this part\n",
    "\n",
    "* Understand the basics of semantic segmentation\n",
    "* Understand the basics of U-Net architecture\n",
    "* Understand how does one predict on prostate images using a pretrained model (in next part, you will learn how to train a model from scratch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements:\n",
    "\n",
    "I have made a python package that you can install with the below command. This package will also automatically install all the libraries needed for this tutorial. If you wanto see how the functions and the classes are implemented, you can check the source code in [unet_explainer](https://github.com/osbm/unet_explainer/) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a public repository, please inspect the code if you are curious about the implementation details.\n",
    "!pip install git+https://github.com/osbm/unet_explainer.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import all the used built-in, custom and third-party libraries to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import our functions and classes\n",
    "from unet_pytorch import (\n",
    "    ProstateDataset,     # Our custom dataset class\n",
    "    print_model_info,    # Function to print pytorch model info\n",
    "    predict,             # Function to predict on a single image\n",
    "    set_seed,            # Function to set seed for reproducibility\n",
    "    plot_predictions,    # Function to plot predictions\n",
    "    plot_one_example,    # Function to plot one example\n",
    ")\n",
    "\n",
    "# third party libraries\n",
    "import torch                                  # pytorch deep learning framework\n",
    "from torch.utils.data import DataLoader       # dataloader class from pytorch to load data\n",
    "\n",
    "import monai                                  # monai medical imaging framework (built on top of pytorch)\n",
    "from monai.networks.nets import UNet          # unet model from monai (there are other models that you use with a single line)\n",
    "\n",
    "import albumentations as A                    # albumentations library for image augmentation and preprocessing\n",
    "from albumentations.pytorch import ToTensorV2 # albumentations class to convert images to tensors\n",
    "\n",
    "# built-in libraries\n",
    "import os  # os library to work with files and directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset and the pretrained model\n",
    "\n",
    "I have put the preprocessed version of the dataset and the pretrained model in a public huggingface dataset repository. You can download them with the below commands.\n",
    "\n",
    "One thing you need to know about this dataset is that i have applied some preprocessing on this dataset using [this script](https://github.com/osbm/unet_explainer/blob/main/scripts/preprocess_data.py). To summarize, the original prostate158 dataset has 3d volume image and masks of patients. But our model is doing 2d segmentation. So i have filtered all the slices that its masks contain segmentation masks below 6 percent. This is an arbitrary number, you can play around with this number to create new datasets.\n",
    "\n",
    "The point is if a slice does not contain *enough* of the prostate, we do not want to use it for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'): # download data if it doesn't exist\n",
    "    # download data\n",
    "    !wget -q https://huggingface.co/datasets/osbm/unet-explainer-data/resolve/main/data.zip\n",
    "    # unzip data\n",
    "    !unzip -q data.zip\n",
    "\n",
    "# -q flag means quiet, so you won't see any output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also download our pretrained model\n",
    "!wget -q https://huggingface.co/datasets/osbm/unet-explainer-data/resolve/main/best_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "\n",
    "We should set an RNG seed for reproducibility. This way we can get the same results on each run. This is important for debugging and comparing different models. Also is useful if you want to prove that you didnt forge your results.\n",
    "\n",
    "> Warning: Total deterministic behavior is not guaranteed between PyTorch releases, individual commits or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds. For this reason it is recommended to also share python version, exact PyTorch version and platform (OS, GPU etc.) when reporting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "There are couple of transformations that we need to apply to our images and masks. We need to resize them into a fixed size, we need to convert them into tensors and we need to normalize them. We will use the albumentations library for this purpose. Albumentations is a popular library for image augmentation.\n",
    "\n",
    "The most common transformation libraries are:\n",
    "- Torchvision (native to pytorch)\n",
    "- Albumentations\n",
    "- Monai (mostly for medical images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "valid_transforms = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "# all we are doing here is resizing the image to 256x256 and converting it to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and dataloader\n",
    "\n",
    "In pytorch, we need to create a custom dataset class. And then we need to create a dataloader class that will use our custom dataset class. This is a common pattern in pytorch.\n",
    "\n",
    "A pytorch dataset tells the number of samples in the dataset and how to get just ONE sample from the dataset. It also tells how to apply transformations to the sample. This is where you want to apply most of your data processing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ProstateDataset(folder='data/test', transform=valid_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds) # number of samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image, example_mask = test_ds[24]\n",
    "example_image.shape, example_mask.shape # shapes will be [channels, height, width] for both image and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see our example sample\n",
    "plot_one_example(example_image, example_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A pytorch dataloader tells how to get a batch of samples from the dataset.\n",
    "You can;\n",
    "- specify the batch size\n",
    "- the number of workers to use for loading the data\n",
    "- select to shuffle the data or not\n",
    "- etc.\n",
    "\n",
    "### What is a batch?\n",
    "\n",
    "You can train a model with one example at a time. But it is not efficient. So we use batches to compute gradients of all the samples in the batch at once. Then we update the model weights with the average of the gradients of all the samples in the batch. This also helps with generalization. Because we are averaging out the noise in the gradients.\n",
    "\n",
    "A batch is a collection of samples. For example, if you have 1000 samples and you set the batch size to 10, then you will have 100 batches. Each batch will contain 10 samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the juicy part. We will use a pretrained U-Net model to predict on our dataset. U-Net is a popular architecture for semantic segmentation. It is a convolutional neural network that is used for image segmentation. It was first introduced by Olaf Ronneberger, Philipp Fischer, and Thomas Brox in the paper U-Net: Convolutional Networks for Biomedical Image Segmentation in 2015.\n",
    "\n",
    "A U-Net consists of two parts:\n",
    "- Contracting path (left side of the U)\n",
    "- Expanding path (right side of the U)\n",
    "\n",
    "![unet-architecture](./assets/unet-architecture.png)\n",
    "\n",
    "This picture looks a bit complicated. But it is not. Let's break it down.\n",
    "\n",
    "The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expanding path combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path.\n",
    "\n",
    "While getting lower in the network, number of channels are increased and the height and width of the image is decreased.\n",
    "\n",
    "But we are not completely losing the spatial information. We are storing the spatial information in the expanding path. We are concatenating the feature maps from the contracting path to the feature maps in the expanding path. This way we are combining the feature maps from the contracting path with the spatial information from the expanding path.\n",
    "\n",
    "Also one hidden benefit of using skip connections is that it helps with vanishing gradient problem. This problem is mostly encountered in deep neural networks. \n",
    "\n",
    "<!-- ![vanishing-gradient-problem](./assets/vanishing-gradients.jpeg) -->\n",
    "\n",
    "<img src=\"./assets/vanishing-gradients.jpeg\" alt=\"vanishing-gradient-problem\" width=\"500\"/>\n",
    "\n",
    "The gradient signal goes through many layers during backpropagation, and the gradient signal becomes smaller and smaller as it goes deeper and reaches the first layers (closer to the input layer). This is called the vanishing gradient problem. This problem makes training deep networks hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    spatial_dims=2, # 2d image\n",
    "    in_channels=1,  # we only used  T2 weighed MRI images\n",
    "    out_channels=3, # 3 labels\n",
    "    channels=[16, 32, 64, 128, 256, 512], # number of channels to use while contracting\n",
    "    strides=(2, 2, 2, 2, 2), # CNN strides\n",
    "    num_res_units=4, # residual connections\n",
    "    dropout=0.15, # dropout rate\n",
    ")\n",
    "# the monai library returns a pytorch model, so we can use it as a pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now our model class only has the logic and operations, we need to load the weights\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a device to run the model\n",
    "\n",
    "A device is used for accelerating the training process. You need to be aware of your devices memory capacity. This notebook runs smoothly in **Colab T4 GPU**.\n",
    "\n",
    "Pytorch supports both CPU and GPU. You can select which device to use with the below code. If you have a GPU, you should use it. Because it is much faster than CPU. But if you don't have a GPU, you can still run this notebook on CPU. We are just making inference after all.\n",
    "\n",
    "> You can also use a Apple MPS (Metal Performance Shaders) to train models on Apple devices (especially if you have M1 or M2 chips instead of GPU cards). Simply change below line with:\n",
    "\n",
    "```python\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets move the model weights and biases to the device memory\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make the model prediction on the test set using the predict function\n",
    "x, y, y_hat = predict(\n",
    "    model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    final_activation=\"softmax\",\n",
    "    calculate_scores=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(x, y, y_hat, num_examples_to_plot=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
