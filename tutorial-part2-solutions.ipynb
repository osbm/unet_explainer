{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](./assets/banner.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can open this notebook in Colab by clicking the Colab icon. Colab provides GPU for free. You can also run this notebook locally by installing the dependencies listed in `requirements.txt`.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osbm/unet_explainer/blob/main/tutorial-part2-solutions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "The second segment of this hands-on workshop aims to provide an in-depth understanding of the renowned U-Net deep learning architecture, specifically tailored for the segmentation of 2D multi-slice prostate MR images. For this exercise, we will be utilizing the [prostate-158 train dataset](https://zenodo.org/record/6481141), which comprises 139 MRI images, and the [prostate-158 test dataset](https://zenodo.org/record/6592345), containing 19 MRI images.\n",
    "\n",
    "**Note for Medical Professionals:** The U-Net architecture is particularly advantageous in medical imaging for its efficiency in segmenting intricate structures, which is a crucial step in the diagnostic and treatment planning process.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* To acquaint participants with the process of coding a deep learning segmentation method using the PyTorch library.\n",
    "* To evaluate the performance impact of various key hyper-parameters on a U-Net model.\n",
    "\n",
    "**Clinical Relevance:** Understanding the influence of hyper-parameters can be instrumental in fine-tuning the model for specific clinical applications, thereby potentially improving diagnostic accuracy and patient outcomes.\n",
    "\n",
    "By adding these notes, you can better engage with your audience by outlining the practical relevance and implications of the material. Would you like to proceed with the next cell?\n",
    "\n",
    "### Our works\n",
    "\n",
    "- Karagoz, Ahmet, et al. \"Anatomically guided self-adapting deep neural network for clinically significant prostate cancer detection on bi-parametric MRI: a multi-center study.\" Insights into Imaging 14.1 (2023): 1-11. https://doi.org/10.1186/s13244-023-01439-0\n",
    "\n",
    "- Karagoz, Ahmet, et al. \"Prostate Lesion Estimation using Prostate Masks from Biparametric MRI.\" arXiv preprint arXiv:2301.09673 (2023). https://doi.org/10.48550/arXiv.2301.09673"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Required Software and Libraries:\n",
    "\n",
    "For your convenience, I have created a Python package that encapsulates all the necessary dependencies and libraries required for this tutorial. You can install this package using the command provided below. By doing so, you will automatically install all the libraries that are essential for this workshop.\n",
    "\n",
    "For those interested in understanding the underlying implementation of the functions and classes used in this tutorial, the source code is publicly available in the [unet_explainer GitHub repository](https://github.com/osbm/unet_explainer/).\n",
    "\n",
    "#### Note to Participants:\n",
    "\n",
    "1. **Why a Custom Python Package?**: Packaging the required libraries and dependencies into a single installable unit simplifies the setup process, allowing you to focus more on the tutorial's content rather than troubleshooting installation issues.\n",
    "\n",
    "2. **Transparency and Extensibility**: The availability of the source code in a public repository offers transparency and provides an opportunity for future customization and improvement. You are encouraged to explore the repository to gain deeper insights into the functionalities provided.\n",
    "\n",
    "By adhering to the above installation steps, you ensure a smooth and efficient setup, allowing us to dive straight into the core topics of this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder on the U-Net architecture\n",
    "\n",
    "U-Net is based on a two-stage convolutional network architecture. The first part, known as the encoder, is similar to conventional CNNs and extracts high-level information. The second part is the decoder, which uses information from the encoder and applies a set of convolutions and upsampling operations to gradually transform feature maps with the purpose of reconstructing segmentation maps at the resolution of the imput image. U-Net architecture also integrates skip connections between the encoder and decoder parts with the goal of retrieving details that were potentially lost during the downsampling while also stabilizing the learning procedure. An illustration of the network architecture is given below.\n",
    "\n",
    "\n",
    "![unet-architecture](https://github.com/osbm/unet_explainer/blob/main/assets/unet-architecture.png?raw=1)\n",
    "\n",
    "The U-Net architecture can be defined through the following main parameters:\n",
    "- the number of feature maps at the first level\n",
    "- the number of levels\n",
    "- the use of the batch normalizations at each level\n",
    "- the type of activation functions\n",
    "- the use of dropout operations\n",
    "- the use of data augmentation\n",
    "\n",
    "The performance of deep learning model also depends on the optimization conditions that were used during the learning process, the main ones being:\n",
    "- the optimization algorithm (*ADAM* and *RMSprop* being among the most popular)\n",
    "- the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a public repository, please inspect the code if you are curious about the implementation details.\n",
    "!pip install git+https://github.com/osbm/unet_explainer.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import all the used built-in, custom and third-party libraries to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import our functions and classes\n",
    "from unet_pytorch import (\n",
    "    ProstateDataset,          # our dataset class\n",
    "    print_model_info,         # function to print pytorch model info\n",
    "    fit_model,                # function to train and validate pytorch model\n",
    "    predict,                  # function to predict on pytorch model\n",
    "    set_seed,                 # helper function to set seed for reproducibility\n",
    "    plot_overlay_4x4,         # helper function to plot 4x4 grid of images\n",
    "    plot_predictions,         # helper function to plot predictions\n",
    "    plot_one_example,         # helper function to plot one example\n",
    "    plot_history,             # helper function to plot training history\n",
    "    plot_comparison_examples, # helper function compare augmented and original images\n",
    ")\n",
    "# third party libraries\n",
    "import torch                                  # pytorch deep learning framework\n",
    "from torch.utils.data import DataLoader       # dataloader class from pytorch to load data\n",
    "\n",
    "import monai                                  # monai medical imaging framework (built on top of pytorch)\n",
    "from monai.networks.nets import UNet          # unet model from monai (there are other models that you use with a single line)\n",
    "\n",
    "import albumentations as A                    # albumentations library for image augmentation and preprocessing\n",
    "from albumentations.pytorch import ToTensorV2 # albumentations class to convert images to tensors\n",
    "\n",
    "# built-in libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets download the data\n",
    "\n",
    "I have uploaded a preprocessed version of the data on a huggingface dataset. You can download it by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'): # download data if it doesn't exist\n",
    "    # download data\n",
    "    !wget -q https://huggingface.co/datasets/osbm/unet-explainer-data/resolve/main/data.zip\n",
    "    # unzip data\n",
    "    !unzip -q data.zip\n",
    "\n",
    "# -q flag means quiet, so you won't see any output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "\n",
    "We should set an RNG seed for reproducibility. This way we can get the same results on each run. This is important for debugging and comparing different models. Also is useful if you want to prove that you didnt forge your results.\n",
    "\n",
    "> Warning: Total deterministic behavior is not guaranteed between PyTorch releases, individual commits or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds. For this reason it is recommended to also share python version, exact PyTorch version and platform (OS, GPU etc.) when reporting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "This is a weird step. We are purposely degrading the quality of the data. This is done to make the model more robust to noise and other artifacts. There is also randomness in the augmentation. This way the model will see different images in each epoch. This is done to prevent overfitting. We are basically increasing the size of the dataset for free by augmenting the data.\n",
    "\n",
    "![augmentation](./assets/data-augmentation.jpeg)\n",
    "\n",
    "Look at all these images. You would want your model to be able to detect the cat in all of these images. This is what data augmentation does. It makes the model more robust to noise and other artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Augmentation Code\n",
    "\n",
    "Here's the code snippet for data preprocessing and augmentation using Albumentations library:\n",
    "\n",
    "```python\n",
    "image_size = 128\n",
    "train_transforms = A.Compose([ # data augmentation and preprocessing pipeline\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    A.HorizontalFlip(p=0.5), # probability of 0.5\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=5, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transforms = A.Compose([ # only data preprocessing pipeline\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "```\n",
    "\n",
    "**Technical Note:** The `train_transforms` pipeline includes a series of data augmentation techniques such as horizontal flip, vertical flip, and rotation, among others. These augmentations are critical for enhancing the model's robustness to variations in the data. The `valid_transforms` pipeline, on the other hand, is solely for data resizing and tensor conversion and does not include data augmentation.\n",
    "\n",
    "**Clinical Insight:** Data augmentation methods like flipping and rotation are especially useful in medical imaging to simulate different orientations and positions of anatomical structures. This helps in training a more generalized model, which is crucial for real-world applications where variability is inherent.\n",
    "\n",
    "Would you like to review the next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "train_transforms = A.Compose([ # data augmentation and preprocessing pipeline\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    A.HorizontalFlip(p=0.5), # probability of 0.5\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=5, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transforms = A.Compose([ # only data preprocessing pipeline\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create our dataset objects\n",
    "train_ds = ProstateDataset(folder='data/train', transform=train_transforms)\n",
    "valid_ds = ProstateDataset(folder='data/valid', transform=valid_transforms)\n",
    "test_ds = ProstateDataset(folder='data/test', transform=valid_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader Configuration Code\n",
    "\n",
    "Here is the code for setting up DataLoader objects for the training, validation, and testing datasets:\n",
    "\n",
    "```python\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "```\n",
    "\n",
    "**Technical Note:** The `DataLoader` objects are configured with a batch size of 16. The training dataset (`train_loader`) is set to shuffle the data before each epoch, which helps in breaking any correlations in the sequence of input data and thereby improves model training. For the validation (`valid_loader`) and test (`test_loader`) datasets, shuffling is disabled as it is not generally required during the evaluation phase.\n",
    "\n",
    "**Clinical Insight:** The choice of batch size can have a significant impact on both the training speed and the performance of the model. A smaller batch size may offer a more precise estimate of the gradient, but it may also require more iterations to converge. Clinical practitioners should be aware that these hyperparameters can be adjusted based on the specific computational and clinical requirements.\n",
    "\n",
    "Would you like to continue to the next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# and also create our dataloader objects\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see augmented training data and compare it with the un-augmented validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape) # shape = [batch_size, channels, height, width]\n",
    "    print(y.shape) # shape = [batch_size, channels, height, width]\n",
    "    plot_one_example(x[0], y[0])\n",
    "    print(\"Overlay examples:\")\n",
    "    plot_overlay_4x4((x, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how the train set is looking let us compare a random example from the validation set and see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_example = train_ds[42]\n",
    "valid_example = valid_ds[42]\n",
    "\n",
    "plot_comparison_examples(train_example, valid_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration Code\n",
    "\n",
    "Below is the code snippet for configuring the U-Net model with specific hyperparameters:\n",
    "\n",
    "```python\n",
    "model = UNet( # these are the hyperparameters we can change\n",
    "    spatial_dims=2, # 2D image\n",
    "    in_channels=1,  # we only used T2-weighted MRI images\n",
    "    out_channels=3, # 3 labels\n",
    "    channels=[16, 32, 64, 128, 256, 512],\n",
    "    strides=(2, 2, 2, 2, 2), # CNN strides\n",
    "    num_res_units=4, # residual connections\n",
    "    dropout=0.15, # dropout rate\n",
    ")\n",
    "print_model_info(model)\n",
    "```\n",
    "\n",
    "**Technical Note:** The `UNet` model is initialized with various hyperparameters like spatial dimensions, input and output channels, the number of filters at each layer, strides for the convolutions, number of residual units, and the dropout rate. Each of these parameters can significantly impact the model's performance and should be carefully chosen.\n",
    "\n",
    "**Clinical Insight:** The model is tailored for 2D T2-weighted MRI images with three distinct labels. This specific configuration can be useful for segmenting different zones or tissues within the prostate gland. Understanding the role of each hyperparameter can help in customizing the model for specific clinical tasks, thereby enhancing its utility in practice.\n",
    "\n",
    "Would you like to proceed to the next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = UNet( # these are the hyperparameters we can change\n",
    "    spatial_dims=2, # 2d image\n",
    "    in_channels=1,  # we only used  T2 weighed MRI images\n",
    "    out_channels=3, # 3 labels\n",
    "    channels=[16, 32, 64, 128, 256, 512],\n",
    "    strides=(2, 2, 2, 2, 2), # CNN strides\n",
    "    num_res_units=4, # residual connections\n",
    "    dropout=0.15, # dropout rate\n",
    ")\n",
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "An optimizer is an algorithm that is used to update the weights of the neural network. The most popular optimizers are Adam, SGD, RMSprop, etc.\n",
    "\n",
    "<!-- ![optimizers](./assets/optimizer.jpeg) -->\n",
    "\n",
    "<img src=\"./assets/optimizer.jpeg\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Lets imagine above image. This example neural network has only 2 parameters. For all these parameters we have different loss values. And these loss values are represented by height. Optimizer is trying to find the global minima. It does this by taking small steps in the direction of the steepest slope. This is called gradient descent. There are different types of gradient descent. The most popular one is Adam.\n",
    "\n",
    "This is just a visualization for 2 parametered model. Now imagine 60 million parameters. That is an huge space to search. We cant just simply search the whole space. Our models have to take small steps. If the steps are too small then it will take a long time to reach the global minima. If the steps are too big then we might miss and jump around the global minima back and forth.\n",
    "\n",
    "This is one of the reasons there are things like learning rate schedulers. They change the learning rate over time or dependent on the current loss. This way we can take big steps in the beginning and small steps in the end. This way we can reach the global minima faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# we will use the Adam optimizer with a learning rate of 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "Loss function is a function that is used to calculate the error of the model. It gives the answer to the \"How much did the model get wrong?\" question. \n",
    "\n",
    "There are different types of loss functions. It is often defined by the task. For example, for classification tasks we use cross entropy loss. For regression tasks we use mean squared error loss. For segmentation tasks we use dice loss.\n",
    "\n",
    "## Dice Loss\n",
    "\n",
    "Dice loss is a loss function that is used for segmentation tasks. It is defined as follows:\n",
    "\n",
    "![loss](./assets/dice-loss.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# we will use the Dice loss function from monai. You can also use other compount loss functions from monai\n",
    "loss = monai.losses.DiceLoss(include_background=True, to_onehot_y=True, softmax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Code\n",
    "\n",
    "Here is the code for transferring the model weights to the GPU (if available) and initiating the training process:\n",
    "\n",
    "```python\n",
    "# Move model weights to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "model, history = fit_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    device=device,\n",
    "    epochs=30,\n",
    ")\n",
    "```\n",
    "\n",
    "**Technical Note:** The model is first moved to the GPU using `torch.device`, which ensures that all computations are performed on the GPU if available, thereby speeding up the training process. The training is then initiated using the `fit_model` function, where various parameters like loaders, optimizer, loss function, and the number of epochs are passed.\n",
    "\n",
    "**Clinical Insight:** Moving the model to a GPU can significantly accelerate the training time, which is especially crucial for medical applications where timely diagnosis and intervention can be life-saving. The choice of loss function and optimizer also has implications for the model's ability to generalize well to new, unseen medical data. Practitioners should be aware that the number of epochs is another tunable parameter that needs to be optimized based on the performance requirements and available computational resources.\n",
    "\n",
    "Would you like to proceed to the next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model weights to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss here if not already defined\n",
    "# optimizer = ...\n",
    "# loss = ...\n",
    "\n",
    "# Custom fit_model function or equivalent should be defined here to ensure compatibility\n",
    "# def fit_model(...):\n",
    "#     ...\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    model, history = fit_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        device=device,\n",
    "        epochs=2,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # Add any debugging or logging code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see our training history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our function is keeping track of the best model based on validation loss\n",
    "# just so that we can load the model in its best state, not after we realized that it started overfitting\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how our model performs on the test set\n",
    "x, y, y_hat = predict(model, test_loader=test_loader, device=device, final_activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the entire test set with predictions\n",
    "print(x.shape, y.shape, y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(x, y, y_hat, num_examples_to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE\n",
    "\n",
    "Now you know roughly how U-Net training workflow works.\n",
    "You can still learn so much just by playing around with:\n",
    "- other segmentation models\n",
    "- other data augmentation techniques\n",
    "- other loss functions\n",
    "- other optimizers\n",
    "- other learning rate schedulers\n",
    "\n",
    "But I think this is enough for now. I hope you enjoyed this tutorial. If you have any questions or suggestions, please feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
