{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can open this notebook in Colab by clicking the Colab icon. Colab provides GPU for free. You can also run this notebook locally by installing the dependencies listed in `requirements.txt`.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osbm/unet_explainer/blob/main/tutorial-part2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "The second part of this hands-on will allow you to study in depth the structure and performance of the popular deep learning U-Net architecture, in the context of 2D multi-slice prostate MR images segmentation. You will use the (prostate-158 train dataset)[https://zenodo.org/record/6481141] (139 mri images) and (prostate-158 test dataset)[https://zenodo.org/record/6592345] dataset (19 mri images).\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* Study how to code a deep learning segmentation method with the pytorch library.\n",
    "* Compare the performance of a U-Net model based on the choice of key hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder on the U-Net architecture\n",
    "\n",
    "U-Net is based on a two-stage convolutional network architecture. The first part, known as the encoder, is similar to conventional CNNs and extracts high-level information. The second part is the decoder, which uses information from the encoder and applies a set of convolutions and upsampling operations to gradually transform feature maps with the purpose of reconstructing segmentation maps at the resolution of the imput image. U-Net architecture also integrates skip connections between the encoder and decoder parts with the goal of retrieving details that were potentially lost during the downsampling while also stabilizing the learning procedure. An illustration of the network architecture is given below.\n",
    "\n",
    "\n",
    "![unet-architecture](./assets/unet-architecture.png)\n",
    "\n",
    "The U-Net architecture can be defined through the following main parameters:\n",
    "- the number of feature maps at the first level\n",
    "- the number of levels\n",
    "- the use of the batch normalizations at each level\n",
    "- the type of activation functions\n",
    "- the use of dropout operations\n",
    "- the use of data augmentation\n",
    "\n",
    "The performance of deep learning model also depends on the optimization conditions that were used during the learning process, the main ones being:\n",
    "- the optimization algorithm (*ADAM* and *RMSprop* being among the most popular)\n",
    "- the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements:\n",
    "\n",
    "I have made a python package that you can install with the below command. This package will also automatically install all the libraries needed for this tutorial. If you wanto see how the functions and the classes are implemented, you can check the source code in [unet_explainer](https://github.com/osbm/unet_explainer/) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a public repository, please inspect the code if you are curious about the implementation details.\n",
    "!pip install git+https://github.com/osbm/unet_explainer.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import all the used built-in, custom and third-party libraries to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import our functions and classes\n",
    "from unet_pytorch import (\n",
    "    ProstateDataset,          # our dataset class\n",
    "    print_model_info,         # function to print pytorch model info\n",
    "    fit_model,                # function to train and validate pytorch model\n",
    "    predict,                  # function to predict on pytorch model\n",
    "    set_seed,                 # helper function to set seed for reproducibility\n",
    "    plot_overlay_4x4,         # helper function to plot 4x4 grid of images\n",
    "    plot_predictions,         # helper function to plot predictions\n",
    "    plot_one_example,         # helper function to plot one example\n",
    "    plot_history,             # helper function to plot training history\n",
    "    plot_comparison_examples, # helper function compare augmented and original images\n",
    ")\n",
    "# third party libraries\n",
    "import torch                                  # pytorch deep learning framework\n",
    "from torch.utils.data import DataLoader       # dataloader class from pytorch to load data\n",
    "\n",
    "import monai                                  # monai medical imaging framework (built on top of pytorch)\n",
    "from monai.networks.nets import UNet          # unet model from monai (there are other models that you use with a single line)\n",
    "\n",
    "import albumentations as A                    # albumentations library for image augmentation and preprocessing\n",
    "from albumentations.pytorch import ToTensorV2 # albumentations class to convert images to tensors\n",
    "\n",
    "# built-in libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets download the data\n",
    "I have uploaded a preprocessed version of the data on a huggingface dataset. You can download it by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    !wget -q https://huggingface.co/datasets/osbm/unet-explainer-data/resolve/main/data.zip\n",
    "    !unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should set RNG seed for reproducibility. This way we can get the same results on each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "This is a weird step. We are purposely degrading the quality of the data. This is done to make the model more robust to noise and other artifacts. There is also randomness in the augmentation. This way the model will see different images in each epoch. This is done to prevent overfitting. We are basically increasing the size of the dataset for free by augmenting the data.\n",
    "\n",
    "![augmentation](./assets/data-augmentation.jpeg)\n",
    "\n",
    "Look at all these images. You would want your model to be able to detect the cat in all of these images. This is what data augmentation does. It makes the model more robust to noise and other artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data augmentations is implemented in albumentations library. You can use any augmentation from the library. You can find the list of augmentations [here](https://albumentations.ai/docs/api_reference/augmentations/transforms/).\n",
    "\n",
    "> Instruction: play with these values to see how they affect the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "train_transforms = A.Compose([ # data augmentation and preprocessing pipeline\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    A.HorizontalFlip(p=0.5), # probability of 0.5\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=5, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transforms = A.Compose([ # only data preprocessing pipeline\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create our dataset objects\n",
    "train_ds = ProstateDataset(folder='data/train', transform=train_transforms)\n",
    "valid_ds = ProstateDataset(folder='data/valid', transform=valid_transforms)\n",
    "test_ds = ProstateDataset(folder='data/test', transform=valid_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and also create our dataloader objects\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see augmented training data and compare it with the un-augmented validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape) # shape = [batch_size, channels, height, width]\n",
    "    print(y.shape) # shape = [batch_size, channels, height, width]\n",
    "    plot_one_example(x[0], y[0])\n",
    "    print(\"Overlay examples:\")\n",
    "    plot_overlay_4x4((x, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how the train set is looking let us compare a random example from the validation set and see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = train_ds[42]\n",
    "valid_example = valid_ds[42]\n",
    "\n",
    "plot_comparison_examples(train_example, valid_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet( # these are the hyperparameters we can change\n",
    "    spatial_dims=2, # 2d image\n",
    "    in_channels=1,  # we only used  T2 weighed MRI images\n",
    "    out_channels=3, # 3 labels\n",
    "    channels=[16, 32, 64, 128, 256, 512],\n",
    "    strides=(2, 2, 2, 2, 2), # CNN strides\n",
    "    num_res_units=4, # residual connections\n",
    "    dropout=0.15, # dropout rate\n",
    ")\n",
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "An optimizer is an algorithm that is used to update the weights of the neural network. The most popular optimizers are Adam, SGD, RMSprop, etc.\n",
    "\n",
    "<!-- ![optimizers](./assets/optimizer.jpeg) -->\n",
    "\n",
    "<img src=\"./assets/optimizer.jpeg\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Lets imagine above image. This example neural network has only 2 parameters. For all these parameters we have different loss values. And these loss values are represented by height. Optimizer is trying to find the global minima. It does this by taking small steps in the direction of the steepest slope. This is called gradient descent. There are different types of gradient descent. The most popular one is Adam.\n",
    "\n",
    "This is just a visualization for 2 parametered model. Now imagine 60 million parameters. That is an huge space to search. We cant just simply search the whole space. Our models have to take small steps. If the steps are too small then it will take a long time to reach the global minima. If the steps are too big then we might miss and jump around the global minima back and forth.\n",
    "\n",
    "This is one of the reasons there are things like learning rate schedulers. They change the learning rate over time or dependent on the current loss. This way we can take big steps in the beginning and small steps in the end. This way we can reach the global minima faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the Adam optimizer with a learning rate of 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "Loss function is a function that is used to calculate the error of the model. It gives the answer to the \"How much did the model get wrong?\" question. \n",
    "\n",
    "There are different types of loss functions. It is often defined by the task. For example, for classification tasks we use cross entropy loss. For regression tasks we use mean squared error loss. For segmentation tasks we use dice loss.\n",
    "\n",
    "## Dice Loss\n",
    "\n",
    "Dice loss is a loss function that is used for segmentation tasks. It is defined as follows:\n",
    "\n",
    "![loss](./assets/dice-loss.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the Dice loss function from monai. You can also use other compount loss functions from monai\n",
    "loss = monai.losses.DiceLoss(include_background=True, to_onehot_y=True, softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model weights to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# start training\n",
    "model, history = fit_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    device=device,\n",
    "    epochs=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see our training history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our function is keeping track of the best model based on validation loss\n",
    "# just so that we can load the model in its best state, not after we realized that it started overfitting\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how our model performs on the test set\n",
    "x, y, y_hat = predict(model, test_loader=test_loader, device=device, final_activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the entire test set with predictions\n",
    "print(x.shape, y.shape, y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(x, y, y_hat, num_examples_to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE\n",
    "\n",
    "Now you know roughly how U-Net training workflow works.\n",
    "You can still learn so much just by playing around with:\n",
    "- other segmentation models\n",
    "- other data augmentation techniques\n",
    "- other loss functions\n",
    "- other optimizers\n",
    "- other learning rate schedulers\n",
    "\n",
    "But I think this is enough for now. I hope you enjoyed this tutorial. If you have any questions or suggestions, please feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
