{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can open this notebook in Colab by clicking the Colab icon. Colab provides GPU for free. You can also run this notebook locally by installing the dependencies listed in `requirements.txt`.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osbm/unet_explainer/blob/main/tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "The second part of this hands-on will allow you to study in depth the structure and performance of the popular deep learning U-Net architecture, in the context of 2D multi-slice prostate MR images segmentation. You will use the (prostate-158 train dataset)[https://zenodo.org/record/6481141] (139 mri images) and (prostate-158 test dataset)[https://zenodo.org/record/6592345] dataset (19 mri images).\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* Study how to code a deep learning segmentation method with the pytorch library.\n",
    "* Compare the performance of a U-Net model based on the choice of key hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder on the U-Net architecture\n",
    "\n",
    "U-Net is based on a two-stage convolutional network architecture. The first part, known as the encoder, is similar to conventional CNNs and extracts high-level information. The second part is the decoder, which uses information from the encoder and applies a set of convolutions and upsampling operations to gradually transform feature maps with the purpose of reconstructing segmentation maps at the resolution of the imput image. U-Net architecture also integrates skip connections between the encoder and decoder parts with the goal of retrieving details that were potentially lost during the downsampling while also stabilizing the learning procedure. An illustration of the network architecture is given below.\n",
    "\n",
    "\n",
    "![unet-architecture](./assets/unet-architecture.png)\n",
    "\n",
    "The U-Net architecture can be defined through the following main parameters:\n",
    "- the number of feature maps at the first level\n",
    "- the number of levels\n",
    "- the use of the batch normalizations at each level\n",
    "- the type of activation functions\n",
    "- the use of dropout operations\n",
    "- the use of data augmentation\n",
    "\n",
    "The performance of deep learning model also depends on the optimization conditions that were used during the learning process, the main ones being:\n",
    "- the optimization algorithm (*ADAM* and *RMSprop* being among the most popular)\n",
    "- the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/osbm/unet_explainer.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_pytorch import ProstateDataset, get_transforms, get_parameter_number, fit_model, predict, set_seed\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from monai.networks.nets import UNet\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms, valid_transforms = get_transforms(image_size=256)\n",
    "\n",
    "train_ds = ProstateDataset(folder='train', transform=train_transforms)\n",
    "valid_ds = ProstateDataset(folder='valid', transform=valid_transforms)\n",
    "test_ds = ProstateDataset(folder='test', transform=valid_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    spatial_dims=2, # 3d image\n",
    "    in_channels=1,  # we only used  T2 weighed MRI images\n",
    "    out_channels=3, # 3 labels\n",
    "    channels=[16, 32, 64, 128, 256, 512],\n",
    "    strides=(2, 2, 2, 2, 2), # CNN strides\n",
    "    num_res_units=4, # residual connections\n",
    "    dropout=0.15, # dropout rate\n",
    ")\n",
    "get_parameter_number(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "loss = monai.losses.DiceLoss(to_onehot_y=True, softmax=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model, history = fit_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    device=device,\n",
    "    epochs=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_pytorch import plot_history\n",
    "\n",
    "plot_history(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
